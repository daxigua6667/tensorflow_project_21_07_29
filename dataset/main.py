# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y95khhxCFocnsMsKoblFj7sxuF6uDNVA
"""

import os
import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
from tensorflow import keras

print(tf.version.VERSION)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
show_dataset = tfds.list_builders()

# dataset = tfds.load("coil100", split="train", data_dir="C:/Users/daxigua/tensorflow_datasets/", download=False)
dataset, dataset_info = tfds.load("coil100", split="train", with_info=True, as_supervised=True, batch_size=-1)
# dataset_test = tfds.load("coil100", split="test")
# dataset_test[:,:] = dataset[0: 30]



# dataset = tfds.load(name="coil100", split="train", data_dir="C:/Users/daxigua/tensorflow_datasets/", download=False)
# assert isinstance(dataset, tf.data.Dataset)

dataset = tfds.as_numpy(dataset)
image = dataset[0]
image = image[:]/255.0
label = dataset[1]
plt.imshow(image[0].astype(np.float32), cmap=plt.get_cmap("gray"))
plt.show()
plt.imshow(image[1].astype(np.float32), cmap=plt.get_cmap("gray"))
plt.show()
plt.imshow(image[2].astype(np.float32), cmap=plt.get_cmap("gray"))
plt.show()
plt.imshow(image[3].astype(np.float32), cmap=plt.get_cmap("gray"))
plt.show()
print(label[0])
print(label[1])
print(label[2])
print(label[3])
# for dataset_example in dataset.take(1):  # 只取一个样本
#     image, label = dataset_example["image"], dataset_example["label"]
#
#     plt.imshow(image.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap("gray"))
#     plt.show()
#     print("Label: %d" % label.numpy())

def normalize_img(image, label):
    """Normalizes images: `uint8` -> `float32`."""
    return tf.cast(image, tf.float32) / 255., label


# dataset = dataset.map(
#     normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
# dataset = dataset.cache()
# dataset = dataset.shuffle(dataset_info.splits['train'].num_examples)
# dataset = dataset.batch(128)
# dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)

# dataset_test_image_normalizaion,  dataset_test_lable_normalizaion= dataset_test["image"]/255.0, dataset["label"]

dataset_builder = tfds.builder("coil100")
dataset_builder.download_and_prepare()
# dataset_train = dataset_builder.as_dataset(split="train")

dataset_train = dataset

# dataset_train = dataset_train.repeat().shuffle(1024).batch(32)

# prefetch 将使输入流水线可以在模型训练时异步获取批处理。
# dataset_train = dataset_train.prefetch(tf.data.experimental.AUTOTUNE)

# 现在你可以遍历数据集的批次并在 mnist_train 中训练批次：
#   ...

info = dataset_builder.info
print("-----------------------------------------------")
print(dataset_info)
print("-----------------------------------------------")

print("-----------------------------------------------")
print("features", dataset_info.features)
print("-----------------------------------------------")
# print("num_classes", dataset_info.features["label"].num_classes)
print("-----------------------------------------------")
# print("names", dataset_info.features["label"].names)
print("-----------------------------------------------")

dataset_test = tfds.load("coil100", split="train")
fig = tfds.show_examples(info, dataset_test)

model = models.Sequential()
model.add(layers.Conv2D(128, (3, 3), activation='relu', input_shape=(128, 128, 3), padding='same'))
model.add(layers.Dropout(rate=0.25))
model.add(layers.MaxPooling2D((3, 3)))
model.add(layers.Conv2D(256, (3, 3), activation='relu'))
model.add(layers.Dropout(rate=0.25))
model.add(layers.MaxPooling2D((2, 2)))
# model.add(layers.Conv2D(256, (3, 3), activation='relu'))


model.add(layers.Flatten())
model.add(layers.Dense(1024, activation='relu'))
model.add(layers.Dropout(rate=0.25))
model.add(layers.Dense(72, activation='softmax'))


model.summary()

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.fit(image, label, epochs=300)

# test_loss, test_acc = model.evaluate(train_image, train_label, verbose=2)
#
# print('\nTest accuracy:', test_acc)
#
# probability_model = tf.keras.Sequential([model,
#                                          tf.keras.layers.Softmax()])
#
# # test_images = train_image[13]
# # test_label = train_label[13]
#
# predictions = probability_model.predict(test_images)
#
# np.argmax(predictions[0])
#
# test_label[0]

model.save_weights("savedmodel\coil100.h5")

from google.colab import files

# with open('example.txt', 'w') as f:
#   f.write('some content')

files.download('savedmodel\coil100.h5')

scores = model.evaluate(image, label, verbose=0)

scores

prediction = model.predict(image)

prediction.shape

prediction = np.argmax(prediction, axis=1)

import pandas as pd

pd.crosstab(label, prediction, rownames=["label"], colnames=["preditcion"])

pd.set_option('display.max_columns', None)   #显示完整的列
pd.set_option('display.max_rows', None)  #显示完整的行

import sklearn

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    image, label,                # x,y是原始数据
    test_size=0.2        # test_size默认是0.25
)  # 返回的是 剩余训练集+测试集

from keras.callbacks import EarlyStopping

earlyStopping = keras.callbacks.EarlyStopping(
    monitor='accuracy',
    patience=8,
    verbose=2,
    mode='auto'
)

model.fit(x_train, y_train, validation_split=0.2, verbose=2, batch_size=128, callbacks=[earlyStopping], epochs=300)

